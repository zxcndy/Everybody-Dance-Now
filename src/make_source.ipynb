{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Bruno Mars - That's What I Like\n",
    "yt = YouTube('https://www.youtube.com/watch?v=PMivT7MJ41M')\n",
    "yt.streams.first().download(save_dir, 'mv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pytube import YouTube\n",
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path('./data/source/')\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "img_dir = save_dir.joinpath('images')\n",
    "img_dir.mkdir(exist_ok=True)\n",
    "\n",
    "#create .png frames from the video\n",
    "cap = cv2.VideoCapture(str(save_dir.joinpath('mv.mp4')))\n",
    "i = -625 #remove the first 25 seconds\n",
    "while(cap.isOpened()):\n",
    "    flag, frame = cap.read()\n",
    "    if flag == False or i == 1000:\n",
    "        break\n",
    "    if (i>=0):\n",
    "        cv2.imwrite(str(img_dir.joinpath(f'{i:05d}.png')), frame)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Pose estimation (OpenPose)'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "openpose_dir = Path('./src/PoseEstimation/')\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(openpose_dir))\n",
    "sys.path.append('./src/utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulding VGG19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/PoseEstimation/rtpose_vgg.py:204: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight, std=0.01)\n",
      "src/PoseEstimation/rtpose_vgg.py:206: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(m.bias, 0.0)\n",
      "src/PoseEstimation/rtpose_vgg.py:209: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.model1_1[8].weight, std=0.01)\n",
      "src/PoseEstimation/rtpose_vgg.py:210: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.model1_2[8].weight, std=0.01)\n",
      "src/PoseEstimation/rtpose_vgg.py:212: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.model2_1[12].weight, std=0.01)\n",
      "src/PoseEstimation/rtpose_vgg.py:213: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.model3_1[12].weight, std=0.01)\n",
      "src/PoseEstimation/rtpose_vgg.py:214: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.model4_1[12].weight, std=0.01)\n",
      "src/PoseEstimation/rtpose_vgg.py:215: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.model5_1[12].weight, std=0.01)\n",
      "src/PoseEstimation/rtpose_vgg.py:216: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.model6_1[12].weight, std=0.01)\n",
      "src/PoseEstimation/rtpose_vgg.py:218: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.model2_2[12].weight, std=0.01)\n",
      "src/PoseEstimation/rtpose_vgg.py:219: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.model3_2[12].weight, std=0.01)\n",
      "src/PoseEstimation/rtpose_vgg.py:220: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.model4_2[12].weight, std=0.01)\n",
      "src/PoseEstimation/rtpose_vgg.py:221: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.model5_2[12].weight, std=0.01)\n",
      "src/PoseEstimation/rtpose_vgg.py:222: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.model6_2[12].weight, std=0.01)\n",
      "\n",
      "\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 1/24 [00:09<03:40,  9.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 2/24 [00:19<03:30,  9.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 3/24 [00:28<03:20,  9.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 4/24 [00:37<03:09,  9.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 5/24 [00:47<02:59,  9.45s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 6/24 [00:56<02:49,  9.40s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 7/24 [01:06<02:39,  9.39s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 8/24 [01:15<02:30,  9.38s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 9/24 [01:24<02:20,  9.36s/it]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 10/24 [01:34<02:11,  9.36s/it]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 11/24 [01:43<02:01,  9.36s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 12/24 [01:52<01:52,  9.35s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 13/24 [02:02<01:42,  9.35s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 14/24 [02:11<01:33,  9.34s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▎   | 15/24 [02:20<01:24,  9.34s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 16/24 [02:30<01:15,  9.43s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 17/24 [02:39<01:06,  9.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 18/24 [02:49<00:57,  9.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 19/24 [02:56<00:44,  8.80s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 20/24 [03:04<00:33,  8.41s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 21/24 [03:11<00:24,  8.02s/it]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 22/24 [03:18<00:15,  7.72s/it]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 23/24 [03:25<00:07,  7.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 24/24 [03:32<00:00,  7.36s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# openpose\n",
    "from coco_eval import get_multiplier, get_outputs\n",
    "#from rtpose_vgg import gopenpose_diret_model\n",
    "from rtpose_vgg import get_model\n",
    "# utils\n",
    "from openpose_utils import remove_noise, get_pose\n",
    "import os\n",
    "\n",
    "weight_name = './src/PoseEstimation/network/weight/pose_model.pth'\n",
    "\n",
    "model = get_model('vgg19')\n",
    "model.load_state_dict(torch.load(weight_name))\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "model.float()\n",
    "model.eval()\n",
    "\n",
    "'''make label images for pix2pix'''\n",
    "test_img_dir = save_dir.joinpath('test_img')\n",
    "test_img_dir.mkdir(exist_ok=True)\n",
    "test_label_dir = save_dir.joinpath('test_label_ori')\n",
    "test_label_dir.mkdir(exist_ok=True)\n",
    "test_head_dir = save_dir.joinpath('test_head_ori')\n",
    "test_head_dir.mkdir(exist_ok=True)\n",
    "\n",
    "pose_cords = []\n",
    "for idx in tqdm(range(len(os.listdir(str(img_dir)))-1)):\n",
    "    img_path = img_dir.joinpath('{:05}.png'.format(idx))\n",
    "    img = cv2.imread(str(img_path))\n",
    "    shape_dst = np.min(img.shape[:2])\n",
    "    oh = (img.shape[0] - shape_dst) // 2\n",
    "    ow = (img.shape[1] - shape_dst) // 2\n",
    "\n",
    "    img = img[oh:oh + shape_dst, ow:ow + shape_dst]\n",
    "    img = cv2.resize(img, (512, 512))\n",
    "    multiplier = get_multiplier(img)\n",
    "    with torch.no_grad():\n",
    "        paf, heatmap = get_outputs(multiplier, img, model, 'rtpose')\n",
    "    r_heatmap = np.array([remove_noise(ht)\n",
    "                          for ht in heatmap.transpose(2, 0, 1)[:-1]]) \\\n",
    "        .transpose(1, 2, 0)\n",
    "    heatmap[:, :, :-1] = r_heatmap\n",
    "    param = {'thre1': 0.1, 'thre2': 0.05, 'thre3': 0.5}\n",
    "    label, cord = get_pose(param, heatmap, paf)\n",
    "    index = 13\n",
    "    crop_size = 25\n",
    "    try:\n",
    "        head_cord = cord[index]\n",
    "    except:\n",
    "        head_cord = pose_cords[-1] # if there is not head point in picture, use last frame\n",
    "\n",
    "    pose_cords.append(head_cord)\n",
    "    head = img[int(head_cord[1] - crop_size): int(head_cord[1] + crop_size),\n",
    "           int(head_cord[0] - crop_size): int(head_cord[0] + crop_size), :]\n",
    "    plt.imshow(head)\n",
    "    plt.savefig(str(test_head_dir.joinpath('pose_{}.jpg'.format(idx))))\n",
    "    plt.clf()\n",
    "    cv2.imwrite(str(test_img_dir.joinpath('{:05}.png'.format(idx))), img)\n",
    "    cv2.imwrite(str(test_label_dir.joinpath('{:05}.png'.format(idx))), label)\n",
    "    if idx % 100 == 0 and idx != 0:\n",
    "        pose_cords_arr = np.array(pose_cords, dtype=np.int)\n",
    "        np.save(str((save_dir.joinpath('pose_source.npy'))), pose_cords_arr)\n",
    "\n",
    "pose_cords_arr = np.array(pose_cords, dtype=np.int)\n",
    "np.save(str((save_dir.joinpath('pose_source.npy'))), pose_cords_arr)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
